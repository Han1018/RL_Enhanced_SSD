{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque, namedtuple\n",
    "from tqdm import tqdm\n",
    "import sysv_ipc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 DQN 模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 16384)  ## 37502+K --> 16384\n",
    "        self.fc2 = nn.Linear(16384, 16384)\n",
    "        self.fc3 = nn.Linear(16384, action_size) ## 16384 --> 12500，每個可能動作的 Q-value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定義 Replay Memory\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        # 將 Transition 加入 memory，*args 會將所有的引數以 tuple 的方式傳入\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # 從 memory 中隨機取樣一批 Transition\n",
    "        return random.sample(self.memory, batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題與討論\n",
    "* 因為 `state_size` 和 `action_size` 都很大，`memory_size` 建議逐步調整為 100,000 → 200,000 → 500,000 → 1,000,000\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, memory_capacity=100000, batch_size=64, gamma=0.99, lr=0.001):\n",
    "        self.state_size = state_size   # 一個 state 的參數個數\n",
    "        self.action_size = action_size # 有多少個可能的 action\n",
    "        self.memory = ReplayMemory(memory_capacity) # 儲存過去的經驗 (transition)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma # discount factor，未來獎勵的衰減率\n",
    "        self.epsilon = 1.0 # exploration rate：epsilon-greedy，隨機選擇 action 的機率\n",
    "        self.epsilon_decay = 10000\n",
    "        self.epsilon_min = 0.1\n",
    "        self.learning_rate = lr\n",
    "        self.policy_net = DQN(state_size, action_size).to(device) # 主要更新的網路，根據 state 選擇 action、計算 Q-value\n",
    "        self.target_net = DQN(state_size, action_size).to(device) # 用來計算 target Q-value，不會更新參數，定期更新為 policy network\n",
    "        self.update_target_model() # 初始化同步兩個網路的權重\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.steps_done = 0   # 紀錄目前走過多少 step，用來更新 epsilon\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # 將 policy_net 的權重複製到 target_net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()   \n",
    "        \n",
    "    def act(self, state, force_random=False):\n",
    "        # 隨著訓練進行，epsilon 會逐漸衰減，採取隨機 action 的機率會降低\n",
    "        eps_threshold = self.epsilon_min + (self.epsilon - self.epsilon_min) * \\\n",
    "            math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # 如果 force_random 為 True，則強制隨機選擇 action，選到不足空間時用到\n",
    "        if force_random:\n",
    "            eps_threshold = 1.0\n",
    "\n",
    "        if np.random.rand() <= eps_threshold:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        available_space_flat = np.array(state['available_space']).flatten().astype(np.float32)\n",
    "        frequency = np.array(state['frequency']).astype(np.float32)\n",
    "        combined_state = np.concatenate((available_space_flat, frequency))\n",
    "\n",
    "        state_tensor = torch.FloatTensor(combined_state).unsqueeze(0).to(device) # 增加 batch 維度，並轉為 tensor\n",
    "        with torch.no_grad():\n",
    "            action_values = self.policy_net(state_tensor)\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 將 transition 存入 replay memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def replay(self):  ## 把 target model 的參數更新上去\n",
    "        # 訓練 agent，從 replay memory 取樣一批資料，更新 Q-function\n",
    "        # 如果 replay memory 的長度小於 batch size，則不進行訓練\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = np.array([np.array(state['available_space']).ravel().tolist() + state['frequency'] for state in batch.state])\n",
    "        next_state_batch = np.array([np.array(state['available_space']).ravel().tolist() + state['frequency'] for state in batch.next_state])\n",
    "        \n",
    "        # state_batch = torch.FloatTensor(batch.state).to(device)\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)\n",
    "        # next_state_batch = torch.FloatTensor(batch.next_state).to(device)\n",
    "        done_batch = torch.FloatTensor(batch.done).to(device)\n",
    "\n",
    "        # 透過 policy network 計算所有 state 的 Q-value\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 計算下一個 state 的最大 Q-value，detach 出來避免在 backpropagation 時更新 target_net 的參數\n",
    "        next_state_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        next_state_values = next_state_values * (1 - done_batch) # 確保 terminate 時不會受到未來的 reward 影響\n",
    "\n",
    "        # 計算預期的 Q value = 實際 reward + 折扣後的 max Q value\n",
    "        expected_state_action_values = reward_batch + (self.gamma * next_state_values)\n",
    "\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad() # backpropagation 前先歸零參數的 gradient\n",
    "        loss.backward()            # 計算 loss 相對於 network 中各參數的 gradient\n",
    "        self.optimizer.step()      # 使用 optimizer (Adam) 更新 network 的參數\n",
    "        # print('loss:', loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.policy_net.load_state_dict(torch.load(path))\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題與討論\n",
    "* input / output 的 offset 是以 page 為單位\n",
    "* input / output 都是 physical address\n",
    "\n",
    "需要從 SSD 獲取的資料\n",
    "1. write data 的 physical address、available space、前 k 筆資料的 frequency、自己的 frequency\n",
    "2. WAF\n",
    "\n",
    "<font color=8DD1F1>簡單流程</font>  \n",
    "**SSD**   \n",
    " ↓ physical address 等 info、waf  \n",
    "**RL**  \n",
    " ↓ physical address  \n",
    "**SSD**  \n",
    " ↓ physical address 等 info、waf  \n",
    "**RL**  \n",
    " ↓ calculate reward  \n",
    "loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading\n",
    "\n",
    "* `flag = 0`：等待 SSD\n",
    "* `flag = 1`：SSD 準備好資料，收完資料我們要記得改回 `flag = 0`\n",
    "* `flag = 2`：SSD 真正 full，我們要回傳 `result = -1`， 改回 `flag = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import posix_ipc\n",
    "import mmap\n",
    "import threading\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class SSDEnvironment:\n",
    "    def __init__(self, num_physical_blocks, pages_per_block, shm_names, x, y, k=17):\n",
    "        self.num_physical_blocks = num_physical_blocks\n",
    "        self.pages_per_block = pages_per_block\n",
    "        self.x = x # frequency 差異小於 x 時的獎勵\n",
    "        self.y = y # frequency 差異大於 y 時的懲罰\n",
    "        self.k = k # 觀察最近 k 筆資料的 frequency，含自己\n",
    "        self.reward = 0\n",
    "        self.total_pages = self.num_physical_blocks * self.pages_per_block\n",
    "        self.done = False\n",
    "        self.SHM_NAMES = shm_names\n",
    "        self.RESET = -1 # 重置環境\n",
    "\n",
    "        # ----- 整合 transfer 功能: start -----\n",
    "        self.shm1 = posix_ipc.SharedMemory(self.SHM_NAMES['recent_lba_count']) # 前K筆資料freq\n",
    "        self.shm2 = posix_ipc.SharedMemory(self.SHM_NAMES['pca_block_info'])\n",
    "        self.shm3 = posix_ipc.SharedMemory(self.SHM_NAMES['waf'])\n",
    "        self.shm4 = posix_ipc.SharedMemory(self.SHM_NAMES['flag'])\n",
    "        self.shm5 = posix_ipc.SharedMemory(self.SHM_NAMES['result'])\n",
    "        self.shm6 = posix_ipc.SharedMemory(self.SHM_NAMES['self_freq'])\n",
    "\n",
    "        self.map1 = mmap.mmap(self.shm1.fd, self.shm1.size)\n",
    "        self.map2 = mmap.mmap(self.shm2.fd, self.shm2.size)\n",
    "        self.map3 = mmap.mmap(self.shm3.fd, self.shm3.size)\n",
    "        self.map4 = mmap.mmap(self.shm4.fd, self.shm4.size)\n",
    "        self.map5 = mmap.mmap(self.shm5.fd, self.shm5.size)\n",
    "        self.map6 = mmap.mmap(self.shm6.fd, self.shm6.size)\n",
    "\n",
    "        self.shm1.close_fd()\n",
    "        self.shm2.close_fd()\n",
    "        self.shm3.close_fd()\n",
    "        self.shm4.close_fd()\n",
    "        self.shm5.close_fd()\n",
    "        self.shm6.close_fd()\n",
    "        # ----- 整合 transfer 功能: end -----\n",
    "\n",
    "        # state: available_space, frequency\n",
    "        self.available_space = self.initialize_space() # !! 要從虛擬 ssd 拿到\n",
    "        self.frequency = self.initialize_frequency()   # !! 要從虛擬 ssd 拿到\n",
    "        self.current_state = self.get_state()\n",
    "        # reward: waf\n",
    "        self.original_waf = self.initialize_waf()      # !! 要從虛擬 ssd 拿到\n",
    "        self.current_waf = self.initialize_waf()       # !! 要從虛擬 ssd 拿到\n",
    "\n",
    "        self.flag_event = threading.Event() # f開關 flag_event = True 代表有新的資料 flag = 1  \n",
    "        self.thread = threading.Thread(target=self.monitor_flag)\n",
    "        self.thread.start()\n",
    "\n",
    "    # ---------------------- initialize：start ----------------------\n",
    "    def initialize_space(self):\n",
    "        # 初始化 available_space 為所有區塊都是可用的，offset 以 page 為單位\n",
    "        # [(offset, space, block_freq), ...] = [(0, pages_per_block, 0), (pages_per_block, pages_per_block ,0), ...]\n",
    "        space = [(i * self.pages_per_block, self.pages_per_block, 0) for i in range(self.num_physical_blocks)]\n",
    "        return space\n",
    "\n",
    "    def initialize_frequency(self):\n",
    "        # 初始化 frequency 為全 0\n",
    "        return [0] * self.k\n",
    "\n",
    "    def initialize_waf(self):\n",
    "        return 0\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'available_space': self.available_space,\n",
    "            'frequency': self.frequency\n",
    "        }\n",
    "    # ---------------------- initialize：end ----------------------\n",
    "\n",
    "    # ---------------------- 連續讀取 flag 數值：start ----------------------\n",
    "    def monitor_flag(self):\n",
    "        while True:\n",
    "            self.map4.seek(0)\n",
    "            flag = ctypes.c_int.from_buffer_copy(self.map4.read(ctypes.sizeof(ctypes.c_int)))\n",
    "            if flag.value == 1:\n",
    "                self.flag_event.set()\n",
    "            elif flag.value == 2:\n",
    "                self.done = True\n",
    "                self.flag_event.set()\n",
    "            time.sleep(0.1)\n",
    "    # ---------------------- 連續讀取 flag 數值：end ----------------------\n",
    "\n",
    "    # ----------------------  SSD 互動：start ----------------------\n",
    "    def read_ssd_data(self):\n",
    "        MAX_RECENT_K = self.k - 1\n",
    "        PHYSICAL_NAND_NUM = self.num_physical_blocks\n",
    "        while True:  # 優化\n",
    "            # print(\"read_ssd_data\")\n",
    "            self.flag_event.wait()  # flag_event = True 代表有新的資料 flag = 1\n",
    "            if self.done:\n",
    "                break\n",
    "            self.map1.seek(0)\n",
    "            recent_lba_count = (ctypes.c_int * MAX_RECENT_K).from_buffer_copy(self.map1.read(ctypes.sizeof(ctypes.c_int) * MAX_RECENT_K))\n",
    "            self.map2.seek(0)\n",
    "            raw_pca_block_info = (ctypes.c_float * (PHYSICAL_NAND_NUM * 3)).from_buffer_copy(self.map2.read(ctypes.sizeof(ctypes.c_float) * PHYSICAL_NAND_NUM * 3))\n",
    "            pca_block_info = [(raw_pca_block_info[i * 3], raw_pca_block_info[i * 3 + 1], raw_pca_block_info[i * 3 + 2]) for i in range(PHYSICAL_NAND_NUM)]\n",
    "            self.map3.seek(0)\n",
    "            waf = ctypes.c_float.from_buffer_copy(self.map3.read(ctypes.sizeof(ctypes.c_float)))\n",
    "            self.map6.seek(0)\n",
    "            self_freq = ctypes.c_int.from_buffer_copy(self.map6.read(ctypes.sizeof(ctypes.c_int)))\n",
    "\n",
    "            self.available_space = pca_block_info\n",
    "            self.frequency = list(recent_lba_count) + [self_freq.value]\n",
    "            self.current_waf = waf.value\n",
    "            self.current_state = self.get_state()\n",
    "            break\n",
    "\n",
    "    def send_to_ssd(self, action):\n",
    "        self.map5.seek(0)\n",
    "        self.map5.write(ctypes.c_int(action).value.to_bytes(ctypes.sizeof(ctypes.c_int), byteorder='little', signed=True))\n",
    "        self.map5.seek(0)\n",
    "        result = ctypes.c_int.from_buffer_copy(self.map5.read(ctypes.sizeof(ctypes.c_int)))\n",
    "        # print(f\"action: {action}, result: {result.value}\")\n",
    "        self.map4.seek(0)\n",
    "        self.map4.write(ctypes.c_int(0).value.to_bytes(ctypes.sizeof(ctypes.c_int), byteorder='little'))\n",
    "        self.flag_event.clear() # flag_event 設為 False flag = 0\n",
    "    # ---------------------- SSD 互動：end ----------------------\n",
    "\n",
    "    def step(self, action):\n",
    "        # 執行 action 並返回新的狀態、獎勵和是否終止\n",
    "        _, _, block_freq = self.available_space[action]\n",
    "\n",
    "        self.read_ssd_data()\n",
    "        \n",
    "        # 計算獎勵\n",
    "        self.reward = self.calculate_reward(block_freq)\n",
    "\n",
    "        # 算完 reward 將 waf 更新\n",
    "        self.original_waf = self.current_waf\n",
    "\n",
    "        # 返回新的狀態、獎勵和是否終止\n",
    "        return self.current_state, self.reward, self.done\n",
    "\n",
    "    def calculate_reward(self, block_freq): \n",
    "        # 計算 WAF\n",
    "        waf_reward = self.calculate_waf_reward()\n",
    "\n",
    "        # 計算 frequency 獎勵\n",
    "        self_freq = self.frequency[-1]\n",
    "        freq_reward = self.calculate_frequency_reward(block_freq, self_freq)\n",
    "        # 總獎勵\n",
    "        self.reward += (0.55 * waf_reward + 0.35 * freq_reward)\n",
    "        return self.reward\n",
    "\n",
    "    def calculate_waf_reward(self):\n",
    "        # 模擬 WAF 計算\n",
    "        waf_change = self.current_waf - self.original_waf\n",
    "        if waf_change < 0:\n",
    "            return 100\n",
    "        else:\n",
    "            return -waf_change * 100\n",
    "\n",
    "    def calculate_frequency_reward(self, block_freq, self_freq):\n",
    "        if block_freq == self_freq:\n",
    "            return 100\n",
    "        elif abs(block_freq - self_freq) < self.x:\n",
    "            return 50\n",
    "        elif abs(block_freq - self_freq) > self.y:\n",
    "            return -100\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def reset(self):\n",
    "        # 重置環境\n",
    "        self.send_to_ssd(self.RESET)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.available_space = self.initialize_space()\n",
    "        self.frequency = self.initialize_frequency()\n",
    "        self.original_waf = self.initialize_waf()\n",
    "        self.read_ssd_data()\n",
    "        return self.get_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Training\n",
    "## 初始化 SSD Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shm_names = {\n",
    "    'recent_lba_count': \"/shm_recent_lba_count\",\n",
    "    'pca_block_info': \"/shm_pca_block_info\",\n",
    "    'waf': \"/shm_waf\",\n",
    "    'flag': \"/shm_flag\",\n",
    "    'result': \"/shm_result\",\n",
    "    'self_freq': \"/shm_self_freq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6   # 前 K 個 + 自己資料的頻率\n",
    "X = 50  # frequency 差異小於 X (+50)\n",
    "Y = 100 # frequency 差異大於 Y (-100)\n",
    "\n",
    "BLOCK_NUM = 7  # 扣除 1 個 reserve nand\n",
    "PAGES_PER_BLOCK = 20\n",
    "\n",
    "state_size = BLOCK_NUM * 3 + K  # 每個籃子的剩餘容量與寫入位置與頻率，加上當前資料大小和頻率與前 K 個資料的頻率\n",
    "action_size = BLOCK_NUM  # 每個資料的寫入位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|██████████| 100/100 [36:27<00:00, 21.87s/it, reward=3904.67, action=6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練完成\n",
      "權重已儲存到 weight/dqn_weights.pth\n"
     ]
    }
   ],
   "source": [
    "env = SSDEnvironment(BLOCK_NUM, PAGES_PER_BLOCK, shm_names, X, Y, K)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "num_episodes = 100\n",
    "model_weights_path = \"weight/dqn_weights.pth\"\n",
    "\n",
    "losses = []\n",
    "total_actions = []\n",
    "rewards_per_episode = []\n",
    "progress_bar = tqdm(range(num_episodes), desc=\"Training Episodes\")\n",
    "for e in progress_bar:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    actions = []\n",
    "    acc_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        # print('available_space:', state['available_space'][action][1])\n",
    "        retries = 30\n",
    "        while state['available_space'][action][1] == 0 and retries > 0:\n",
    "            # print(action, '重選')\n",
    "            acc_reward -= 50 * 0.1\n",
    "            agent.remember(state, action, acc_reward, state, done)\n",
    "            retries -= 1\n",
    "\n",
    "            action = agent.act(state, force_random=True)\n",
    "            # print('available_space:', state['available_space'][action][1])\n",
    "            if state['available_space'][action][1] > 0:\n",
    "                acc_reward += 50 * 0.1\n",
    "\n",
    "            if retries == 0 and state['available_space'][action][1] == 0:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "        env.send_to_ssd(int(state['available_space'][action][0])) # flag = 0\n",
    "        next_state, reward, done = env.step(action) # flag = 1\n",
    "        reward += acc_reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        loss = agent.replay()\n",
    "        losses.append(loss)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'reward': f'{reward:.2f}',\n",
    "            'action': action\n",
    "        })\n",
    "        if e == num_episodes - 1:\n",
    "            actions.append(action)\n",
    "    rewards_per_episode.append(reward)\n",
    "    total_actions.append(actions)\n",
    "\n",
    "print(\"訓練完成\")\n",
    "# 訓練完成後儲存權重\n",
    "agent.save(model_weights_path)\n",
    "print(f\"權重已儲存到 {model_weights_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
